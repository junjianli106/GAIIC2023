{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4063aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import  BertTokenizer, WEIGHTS_NAME\n",
    "from model.modeling_nezha import NeZhaForSequenceClassification, NeZhaModel\n",
    "from model.configuration_nezha import NeZhaConfig\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup, AutoConfig\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baa7996",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv')\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8909b672",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        super(Config, self).__init__()\n",
    "        \n",
    "        self.SEED = SEED\n",
    "        self.tokenizer_path = 'bert_model/nezha-cn-base'\n",
    "        self.MODEL_PATH = './pretrain/pretrained_0.15_dual/checkpoint-20000'\n",
    "        self.NUM_LABELS = NUM_CLASSES\n",
    "        \n",
    "        # data\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.tokenizer_path) #加载分词模型\n",
    "        self.max_length = 256 #句子最大长度\n",
    "        self.batch_size = 32\n",
    "        \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.full_finetuning = True\n",
    "        self.lr = 3e-5\n",
    "        self.optimizer = 'AdamW'\n",
    "        self.n_warmup = 0\n",
    "        self.save_best_only = True\n",
    "        \n",
    "        self.multi_gpu = False\n",
    "        self.attack = 'pgd'\n",
    "\n",
    "        #self.ema = True\n",
    "        self.flooding = False\n",
    "        self.loss_func = 'ce' # ce dice fl\n",
    "        self.epochs = 20\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca240811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "np.random.seed(config.SEED)\n",
    "seed_everything(seed=config.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332dff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train[['description', 'diagnosis']]\n",
    "print(train_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a754fac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509e9249",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d5abf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAIIC_Dataset(Dataset):\n",
    "    def __init__(self, data_file, input_len, output_len, sos_id=1, eos_id=2, pad_id=0):\n",
    "        super(GAIIC_Dataset, self).__init__()\n",
    "        \n",
    "        with open(data_file, 'r') as fp:\n",
    "            reader = csv.reader(fp)\n",
    "            self.samples = [row for row in reader]\n",
    "            \n",
    "        self.input_len   = input_len\n",
    "        self.output_len  = output_len\n",
    "        self.sos_id      = sos_id\n",
    "        self.eos_id      = eos_id\n",
    "        self.pad_id      = pad_id\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        description = [int(x) for x in self.samples[index][1].split()]\n",
    "        \n",
    "        if len(description) < self.input_len:\n",
    "            description.extend([self.pad_id] * (self.input_len - len(description)))\n",
    "        \n",
    "        # for test\n",
    "        if self.samples[index] < 3:\n",
    "            return np.array(description)[:self.input_len]\n",
    "        \n",
    "        target = [self.sos_id] + [int(x) for x in self.samples[index][2].split()] + [self.eos_id]\n",
    "        if len(target) < self.output_len:\n",
    "            target.extend([self.pad_id] * (self.output_len - len(target)))\n",
    "            \n",
    "        return  np.array(description)[:self.input_len], np.array(target)[:self.output_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a96b84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fca076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nezha_pool_last2emb(nn.Module):\n",
    "    def __init__(self, NeZhaConfig):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        NeZhaConfig.output_hidden_states=True\n",
    "        self.bert = NeZhaModel.from_pretrained(config.MODEL_PATH, config=NeZhaConfig)\n",
    "        self.drop = nn.Dropout(p=0.1)\n",
    "        self.dense1 = nn.Sequential(\n",
    "            nn.Linear(self.bert.config.hidden_size*3, 128),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(self.bert.config.hidden_size*3, NUM_CLASSES)\n",
    "        )\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        hidden_output = outputs[2]\n",
    "        classification_input = torch.cat((pooled_output, hidden_output[-1][:, 0], hidden_output[-2][:, 0]), 1)\n",
    "        output = self.drop(classification_input)\n",
    "        logits = self.dense(output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89665fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ba3247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_fn(model, valid_dataloader, criterion):\n",
    "    val_loss = 0\n",
    "\n",
    "    model.eval()\n",
    "    for step, (input_ids, target_ids) in enumerate(tqdm(valid_dataloader)):\n",
    "        \n",
    "        input_ids = input_ids.to(device)\n",
    "        target_ids = target_ids.to(device)\n",
    "        \n",
    "        pred_ids = model(input_ids=input_ids, taregt_ids=taregt_ids)\n",
    "        \n",
    "        loss = criterion(pred_ids, target_ids)\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    model.train()\n",
    "    avg_val_loss = val_loss / len(valid_dataloader)\n",
    "\n",
    "    return avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032f0e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, train_dataloader, criterion, optimizer, scheduler=None, epoch=0):\n",
    "    \n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for step, (input_ids, target_ids) in enumerate(tqdm(train_dataloader, desc='Epoch ' + str(epoch))):\n",
    "        \n",
    "        input_ids = input_ids.to(device)\n",
    "        target_ids = target_ids.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred_ids = model(input_ids=input_ids, taregt_ids=taregt_ids)\n",
    "        \n",
    "        loss = criterion(pred_ids, target_ids)\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "            \n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "    \n",
    "    print('Training loss:', avg_train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db350d99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58459da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def run(config):\n",
    "    t = time.time()\n",
    "    n_splits = 5\n",
    "    with open(f'folds.json') as f:\n",
    "        kfolds = json.load(f)\n",
    "    \n",
    "    torch.manual_seed(config.SEED)\n",
    "    \n",
    "    for FOLD in range(n_splits):\n",
    "        if config.loss_func == 'dice':\n",
    "            criterion = SelfAdjDiceLoss()\n",
    "        elif config.loss_func == 'ce':\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "        elif  config.loss_func == 'fl':\n",
    "            criterion = FocalLoss(num_class=25)\n",
    "        model = Nezha_pool_last3emb(NeZhaConfig)\n",
    "        \n",
    "        model.to(device)\n",
    "        \n",
    "        if config.multi_gpu:\n",
    "            model = torch.nn.DataParallel(model)\n",
    "            \n",
    "        if config.attack == 'fgm':\n",
    "            fgm = FGM(model, epsilon=1, emb_name='word_embeddings.')\n",
    "        elif config.attack == 'pgd':\n",
    "            pgd = PGD(model)\n",
    "        elif config.attack == 'freelb':\n",
    "            freelb = FreeLB(model)\n",
    "        \n",
    "        train_indices = kfolds[f'fold_{FOLD}']['train']\n",
    "        valid_indices = kfolds[f'fold_{FOLD}']['valid']\n",
    "\n",
    "        train_data = TransformerDataset(train_df, train_indices)\n",
    "        valid_data = TransformerDataset(train_df, valid_indices)\n",
    "\n",
    "        train_dataloader = DataLoader(train_data, batch_size=config.batch_size, num_workers=4, shuffle=True)\n",
    "        valid_dataloader = DataLoader(valid_data, batch_size=config.batch_size, num_workers=4, shuffle=False)\n",
    "        \n",
    "        if config.full_finetuning:\n",
    "            param_optimizer = list(model.named_parameters())\n",
    "            no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "\n",
    "            optimizer_parameters = [\n",
    "                {\n",
    "                    \"params\": [\n",
    "                        p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "                    ],\n",
    "                    \"weight_decay\": 0.01,\n",
    "                },\n",
    "                {\n",
    "                    \"params\": [\n",
    "                        p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "                    ],\n",
    "                    \"weight_decay\": 0.0,\n",
    "                },\n",
    "            ]\n",
    "            optimizer = optim.AdamW(optimizer_parameters, lr=config.lr,  weight_decay=0.01, eps=1e-8)\n",
    "\n",
    "        num_training_steps = len(train_dataloader) * config.epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "\n",
    "        min_avg_val_loss = float('inf')\n",
    "        best_result_epoch = 0\n",
    "        best_val_loss = 0\n",
    "        best_acc = 0\n",
    "        best_acc_epoch = 0\n",
    "        for epoch in range(config.epochs):\n",
    "            train_loss = 0\n",
    "            corrects = 0\n",
    "            for step, batch in enumerate(tqdm(train_dataloader, desc='Epoch ' + str(epoch))):\n",
    "                model.train()\n",
    "\n",
    "                b_input_ids = batch['input_ids'].to(device)\n",
    "                b_attention_mask = batch['attention_mask'].to(device)\n",
    "                b_labels = batch['labels'].to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                logits = model(input_ids=b_input_ids, attention_mask=b_attention_mask)\n",
    "                loss = criterion(logits, b_labels)\n",
    "                if config.flooding:\n",
    "                    loss = abs(loss - 0.4) + 0.4\n",
    "                train_loss += loss.item()\n",
    "                _, preds = torch.max(logits, dim=1)\n",
    "                corrects += torch.sum(preds == b_labels)\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                if config.attack == 'fgm':\n",
    "                    fgm.attack()\n",
    "                    logits_fgm = model(input_ids=b_input_ids, attention_mask=b_attention_mask)\n",
    "                    loss_adv = criterion(logits_fgm, b_labels)\n",
    "                    if config.flooding:\n",
    "                        loss_adv = abs(loss_adv - 0.4) + 0.4\n",
    "                    loss_adv.backward()\n",
    "                    fgm.restore()\n",
    "                elif config.attack == 'pgd':\n",
    "                    pgd.backup_grad()\n",
    "                    # 对抗训练\n",
    "                    K = 3\n",
    "                    for t in range(K):\n",
    "                        pgd.attack(is_first_attack=(t==0)) # 在embedding上添加对抗扰动, first attack时备份param.processor\n",
    "                        if t != K-1:\n",
    "                            model.zero_grad()\n",
    "                        else:\n",
    "                            pgd.restore_grad()\n",
    "                        logits_adv = model(input_ids=b_input_ids, attention_mask=b_attention_mask)\n",
    "                        loss_adv = criterion(logits_adv, b_labels)\n",
    "                        loss_adv.backward() # 反向传播，并在正常的grad基础上，累加对抗训练的梯度\n",
    "                    pgd.restore() # 恢复embedding参数\n",
    "\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "            avg_train_loss = train_loss / len(train_dataloader)\n",
    "            avg_train_acc = corrects.cpu().numpy() / len(train_dataloader) / config.batch_size\n",
    "\n",
    "            print('Training loss:', avg_train_loss, 'Training acc:', avg_train_acc)\n",
    "            #train_fn(model, train_dataloader, valid_dataloader, criterion, optimizer, scheduler, epoch, fgm)\n",
    "            avg_val_loss, avg_val_acc = val_fn(model, valid_dataloader, criterion)\n",
    "\n",
    "            if config.save_best_only:\n",
    "                if best_acc < avg_val_acc:\n",
    "                    best_acc_epoch = epoch\n",
    "                    best_model = copy.deepcopy(model)\n",
    "                    best_acc = avg_val_acc\n",
    "\n",
    "                    model_name = f'{model_type}_{FOLD}'\n",
    "                    state_dict = {k: v for k, v in model.state_dict().items() if 'relative_positions' not in k}\n",
    "                    \n",
    "                    torch.save(state_dict, 'models/' + model_name + '.pt')\n",
    "                    min_avg_val_loss = avg_val_loss\n",
    "\n",
    "            \n",
    "            if epoch - best_acc_epoch > 2:\n",
    "                break\n",
    "    print('Cost time:{}'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028ec407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c8e95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'nezha_mlm_last3cls_0.15_dual_grad_clip'\n",
    "NeZhaConfig = NeZhaConfig.from_pretrained(config.MODEL_PATH)\n",
    "device = config.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbeeb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(config) # 4:19"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "com_envs",
   "language": "python",
   "name": "com_envs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
